<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2C67981H"></script> <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-SZ2C67981H');
    </script> <meta charset=utf-8> <meta http-equiv=X-UA-Compatible content='IE=edge;chrome=1'/> <meta name=viewport content="width=device-width, initial-scale=1"> <link rel=canonical href=https://techblog.thescore.com/2018/06/29/migrating-postgresql-to-aurora-with-near-zero-downtime/> <title>Migrating PostgreSQL to Aurora with near Zero-Downtime | Kevin Jalbert</title> <meta name=description content="The goal here is to migrate an Amazon Web Services Relational Database Service PostgreSQL Database to Aurora with as little downtime as possible. The following documents the initial strategy that w..."/> <meta name=keywords content="database, postgresql, aurora, aws"/> <meta name=datePublished content="2018-06-30 00:00:00 UTC"/> <meta property="og:article:published_time" content="2018-06-30 00:00:00 UTC"/> <meta name=author content="Kevin Jalbert"/> <meta property="og:article:author" content="2018-06-30 00:00:00 UTC"/> <meta name=url content="https://kevinjalbert.com/migrate-postgresql-to-aurora-with-near-zero-downtime/"/> <meta property="og:url" content="https://kevinjalbert.com/migrate-postgresql-to-aurora-with-near-zero-downtime/"/> <meta name=site content="Kevin Jalbert"/> <meta property="og:site_name" content="Kevin Jalbert"/> <meta name="twitter:card" content=summary_large_image /> <meta name="twitter:creator" content="@kevinjalbert"/> <meta name="twitter:description" content="The goal here is to migrate an Amazon Web Services Relational Database Service PostgreSQL Database to Aurora with as little downtime as possible. The following documents the initial strategy that was considered using read replica promotion, followed by the end result using AWS's Database Migration Service."/> <meta name="twitter:image:src" content="https://kevinjalbert.com/images/2018-06-30-migrate-postgresql-to-aurora-with-near-zero-downtime/elephant-migration.jpg"/> <meta name="twitter:site" content="Kevin Jalbert"/> <meta name="twitter:title" content="Migrating PostgreSQL to Aurora with near Zero-Downtime"/> <meta property="og:description" content="The goal here is to migrate an Amazon Web Services Relational Database Service PostgreSQL Database to Aurora with as little downtime as possible. The following documents the initial strategy that was considered using read replica promotion, followed by the end result using AWS's Database Migration Service."/> <meta property="og:image" content="https://kevinjalbert.com/images/2018-06-30-migrate-postgresql-to-aurora-with-near-zero-downtime/elephant-migration.jpg"/> <meta property="og:title" content="Migrating PostgreSQL to Aurora with near Zero-Downtime"/> <script type="application/ld+json">
    {
       "@context": "http://schema.org",
       "@type": "BlogPosting",
       "headline": "Migrating PostgreSQL to Aurora with near Zero-Downtime",
       "keywords": "database, postgresql, aurora, aws",
       "url": "https://kevinjalbert.com/migrate-postgresql-to-aurora-with-near-zero-downtime/",
       "datePublished": "2018-06-30 00:00:00 UTC",
       "dateModified": "2018-06-30 00:00:00 UTC",
       "author": {
         "@type": "Person",
         "name": "Kevin Jalbert",
         "sameAs": [
           "https://twitter.com/kevinjalbert",
           "https://github.com/kevinjalbert",
           "https://keybase.io/kevinjalbert",
           "https://reddit.com/user/kevinjalbert"
         ]
       },
       "mainEntityOfPage": {
          "@type": "WebPage",
         "@id": "https://kevinjalbert.com/migrate-postgresql-to-aurora-with-near-zero-downtime/"
       }
    }
  </script> <link href="/stylesheets/app.css" rel=stylesheet /> <link rel=alternate type="application/atom+xml" title="Articles | Kevin Jalbert" href="/feed.xml"/> </head> <div class=container> <div class=header> <div class="col-xs-12 col-sm-7 navigation"> <a class=banner href="/">Kevin Jalbert</a> <a href="/about/">About</a> <a href="/now/">Now</a> <a href="/uses/">Uses</a> <a href="https://github.com/kevinjalbert/ama/blob/master/README.md">AMA</a> </div> <div class="col-xs-12 col-sm-5 search-component"> <input placeholder=Search class="search ui-autocomplete-input ui-corner-all" autocomplete=on> </div> </div> </div> <body> <div class="container content article"> <div class=row> <div class=col-md-12> <h1 class=article__title><p>Migrating PostgreSQL to Aurora with near Zero-Downtime</p> </h1> <div class="row article__meta-info"> <div class="col-xs-4 col-md-4"> <div class="article__date entry-date">June 30, 2018 </div> </div> <div class="col-xs-4 col-md-4"> <div class=article__read-time>18 min read</div> </div> <div class="col-xs-4 col-md-4"> <div class=article__comments> <a href="https://kevinjalbert.com/migrate-postgresql-to-aurora-with-near-zero-downtime/#disqus_thread">0 Comments</a> </div> </div> </div> <div class="row article__meta-tags"> <div class="col-xs-12 col-md-12"> <div class=article__tags> <a href="/tags/aurora/">aurora</a> <a href="/tags/aws/">aws</a> <a href="/tags/database/">database</a> <a href="/tags/postgresql/">postgresql</a> </div> </div> </div> <img src=/images/2018-06-30-migrate-postgresql-to-aurora-with-near-zero-downtime/elephant-migration.jpg> <i><p><a href="https://flickr.com/photos/jeaneeem/22658750486" title=15_CBP_JMP-39>15<em>CBP</em>JMP-39</a> by <a href="https://flickr.com/people/jeaneeem">jeaneeem</a> is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/2.0/">CC BY-NC-ND</a></p> </i> <div class=article__separator></div> <div class=article__originally-posted-to-notice> <i>~~ Originally posted at <a href=https://techblog.thescore.com/2018/06/29/migrating-postgresql-to-aurora-with-near-zero-downtime/>techblog.thescore.com</a> ~~</i> </div> <div class=newsletter__form> <div class="panel panel-default"> <div class=panel-heading> <h4 class="panel-title accordion-toggle collapsed" data-toggle=collapse data-parent="#accordion" href="#newsletterFormCollapse"> If you enjoy this, consider subscribing to my newsletter </h4> </div> <div id=newsletterFormCollapse class="panel-collapse collapse"> <div class=panel-body> <p style="text-align: center">I aim to publish one article a month</p> <link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel=stylesheet> <style>
              #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
              /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
                 We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
        </style> <div id=mc_embed_signup> <form action="//kevinjalbert.us14.list-manage.com/subscribe/post?u=fe02783eec556cfc2893fe174&amp;id=aecd14f8e6" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class=validate target=_blank novalidate> <div id=mc_embed_signup_scroll> <input type=email value="" name=EMAIL class=email id=mce-EMAIL placeholder="E-mail address" required> <div style="position: absolute; left: -5000px;" aria-hidden=true><input name=b_fe02783eec556cfc2893fe174_aecd14f8e6 tabindex=-1 value=""></div> <div class=clear><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class=button></div> </div> </form> </div> </div> </div> </div> </div> <p>The goal here is to migrate an Amazon Web Services (AWS) Relational Database Service (RDS) PostgreSQL Database to Aurora with as little downtime as possible. The following documents the initial strategy that was considered using read replica promotion, followed by the end result using AWS&rsquo;s Database Migration Service (DMS).</p> <h1>Why Aurora</h1> <p>From the <a href="https://aws.amazon.com/rds/aurora/details/">Aurora Details Page</a>:</p> <blockquote> <p>Amazon Aurora is designed to offer greater than 99.99% availability, increasing MySQL and PostgreSQL performance and availability by tightly integrating the database engine with an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora&rsquo;s storage is fault-tolerant and self-healing and disk failures are repaired in the background without loss of database availability. Amazon Aurora is designed to automatically detect database crashes and restart without the need for crash recovery or to rebuild the database cache. If the entire instance fails, Amazon Aurora will automatically fail over to one of up to 15 read replicas.</p> </blockquote> <p>Essentially, better performance, reliability, and the ability to scale.</p> <p>Our configuration changes become easier. Aurora acts as a cluster, so we have two endpoints to deal with, and the replicas underneath will adjust dynamically. We use <a href="https://github.com/taskrabbit/makara">Makara</a> to spread our database queries between our master and slaves. With RDS PostgreSQL we would have to enumerate all our slave databases so we could take advantage of the replicas. If we added a new replica, we would have to adjust our configuration to take advantage of it. With Aurora, we have two endpoints to deal with now (primary/writer and a read-only URL). These endpoints will dynamically rotate between the available instances that fall under that cluster URL. This is great as it handles a failover to a different primary database, or changing the number of replicas. Effectively, we end up with less configuration, but we still need Makara to direct our write and read queries to the two cluster URLs.</p> <h1>Migration Approaches</h1> <p>As mentioned earlier, there are two approaches to doing this migration. First, we&rsquo;ll cover the <em>recommended</em> approach that AWS suggested as we&rsquo;re on the RDS platform. As we&rsquo;ll cover, it is easy and quick but it does incur downtime during the migration. The second approach uses DMS and was brought up to us after opening a support ticket with AWS on how to do our migration with zero downtime.</p> <h2>Read Replica Promotion</h2> <p>This is the <em>recommended</em> approach for migrating from PostgreSQL to Aurora. It is easy and has a little risk (during the migration), although it has unavoidable downtime.</p> <ul> <li>Create an Aurora read replica off of your master database.</li> <li>Pause the master PostgreSQL database. <ul> <li>This is done so that the no additional write changes are occurring (as it won&rsquo;t be replicated to the Aurora read replica during the promotion process).</li> </ul></li> <li>Promote the Aurora read replica. <ul> <li>This takes about 10-20 minutes (at least in our experience).</li> </ul></li> <li>Deploy app configuration changes to use new Aurora cluster for write/reads.</li> <li>Immediately start building additional read replicas to match existing number. <ul> <li>Each one takes about 5-10 minutes to create (at least in our experience).</li> </ul></li> <li>When happy with the new setup, remove/delete the old databases.</li> </ul> <p>It is <em>recommended</em> to do a dry run of the promotion (and just throw it away afterwards) to gauge how long that process takes.</p> <p>The issue here is that you have:</p> <ul> <li>Downtime due to the promotion process. You need to prevent writes from happening on the master. Replication between the PostgreSQL master and the Aurora read replica breaks when the promotion process starts.</li> <li>Immediately after you have your new Aurora master, you need to start building read replicas up. <ul> <li>In our case, we would want to match our existing number of replicas. So this would take additional time. Otherwise, you possibly run the risk of overwhelming the Database without those read replicas up.</li> </ul></li> </ul> <p>One good thing is that even with the downtime the old read replicas will continue to serve up <code>GET</code> requests. In addition, any CDN/<a href="https://varnish-cache.org/">Varnish</a> caching will also help serve stale data during the downtime.</p> <h2>Full Load and Ongoing Replication with DMS</h2> <p>This approach is much more involved and finicky, but it does offer a <em>near</em> zero-downtime migration strategy. The general process here is:</p> <ul> <li>Create an Aurora read replica (this is so we can keep the DB schema)</li> <li>Promote Aurora read replica so it is on its own</li> <li>Tweak settings on PostgreSQL DB</li> <li>Setup DMS Endpoints for the original PostgreSQL and Aurora databases</li> <li>Setup DMS Replication Instance to assist in the migration</li> <li>Setup DMS Task to use the endpoints and the replication instance</li> <li>Truncate Aurora data (we want to wipe the data, but we keep the schema)</li> <li>Initiate the migration <ul> <li>The first phase is a <em>full load</em> of the tables from the source to the target</li> <li>The second phase is an <em>on-going replication</em> of changes from the source to the target</li> </ul></li> <li>Both DBs are in-sync now</li> <li>Create read replicas off of the Aurora master</li> <li>Change app configuration to point to Aurora cluster</li> <li>Delete old DBs</li> </ul> <p>This is a multi-part process, so the following sections will walk through each part.</p> <h3>Setting up Aurora Read Replica</h3> <ul> <li>Create an Aurora read replica of the PostgreSQL database <ul> <li>Name it something like <code>&lt;service&gt;—&lt;environment&gt;-aurora</code></li> </ul></li> <li>By creating an Aurora instance, we also get an Aurora cluster (i.e., <code>&lt;service&gt;—&lt;environment&gt;-aurora-cluster</code>) <ul> <li>Ensure the subnet group and the security groups are set to the same values as defined for the PostgreSQL DB.</li> </ul></li> <li>You don&rsquo;t need to have multi-AZ here, Aurora will handle fast failover if we have a replica</li> <li>Additional read replicas can then be added later using <code>&lt;service&gt;—&lt;environment&gt;-aurora-&lt;##&gt;</code></li> </ul> <p>At this point, we will have an Aurora read replica that is receiving replicated data changes off the master.</p> <h3>Aurora Read Replica Promotion</h3> <p>To isolate our newly created Aurora read replica so it can be its own master, we&rsquo;re going to promote the replica. This process allows the instance to receive writes, and be the master for other replicas. We need to do this as otherwise we are limited to only one Aurora read replica off of a PostgreSQL master database. By starting the promotion process the on-going replication is broken between the PostgreSQL and Aurora databases.</p> <h3>Tweak the Database Parameters</h3> <p>To enable the ability for PostgreSQL to replicate to Aurora we need to make sure we have certain database parameters set.</p> <h4>Configure PostgreSQL for Replication</h4> <p>The first thing you need to do is change the <code>rds.logical_replication</code> parameter to 1. As described in <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.RDSPostgreSQL">the documentation</a> this parameter actually configures additional related parameters that allow the replication to process:</p> <blockquote> <p>As part of applying this parameter, AWS DMS sets the <code>wal_level</code>, <code>max_wal_senders</code>, <code>max_replication_slots</code>, and <code>max_connections</code> parameters.</p> </blockquote> <p>Another parameter configuration that <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Prerequisites">Amazon recommends</a> is to set <code>wal_sender_timeout</code> to 0.</p> <blockquote> <p>The <code>wal_sender_timeout</code> parameter terminates replication connections that are inactive longer than the specified number of milliseconds. Although the default is 60 seconds, we recommend that you set this parameter to zero, which disables the timeout mechanism.</p> </blockquote> <p>Without setting the timeout parameter, the streaming of Write-Ahead Logging (WAL) files can be terminated if the sender is under load and does not send a WAL file within the timeout period. To prevent any risk of this it is best to disable it during the on-going replication phase of the migration process.</p> <p>There are additional <code>wal</code> settings available, but we <em>shouldn&rsquo;t</em> need to configure them.</p> <h4>Configure Aurora for Replication</h4> <p>On the Aurora instance, we want to limit any foreign key constraints from triggering as the full loads for tables are occurring. Fortunately, if we change the <code>session_replication_role</code> parameter to <code>replica</code>, it only keeps <em>replica</em> related constraint triggers active. This effectively disables foreign key constraints for us.</p> <h3>Create Replication Instance</h3> <p>DMS uses a dedicated <em>replication instance</em> to help facilitate the migration process. In a PostgreSQL to Aurora migration, this machine is responsible for connecting to both the source and target databases and transforming and transferring the data.</p> <p>The following steps expand on what is needed to set up a replication instance for the migration.</p> <ul> <li>Create a replication instance under DMS <ul> <li>Name it as specific as possible for its purpose (i.e., production-api-replication-instance)</li> <li>The <em>Instance Size</em> might need to be tweaked based on various factors (<a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html#CHAP_ReplicationInstance.InDepth">documentation on instance sizes</a>)</li> <li>For example, in our staging we used <code>dms.t2.large</code>, while for production we used <code>dms.c4.large</code></li> <li>Set the <em>VPC</em> to the same as the source and target databases</li> <li>Enable <em>Multi-AZ</em> as it is needed for on-going replication</li> <li>The instance doesn’t need to be <em>Publicly Accessible</em> (as the internal connections are done within the VPC)</li> <li>Ensure the <em>VPC Security Group</em> under <em>Advance</em> is set to <code>default</code></li> <li>This allows for connections between the databases and the replication instance</li> </ul></li> </ul> <h3>Create Source and Target Endpoints</h3> <p>We need to define the <em>endpoints</em> of our databases in DMS. One to represent the source database (PostgreSQL) and another to represent the target database (Aurora). The following steps are to be applied to each endpoint.</p> <ul> <li>The <em>Server Name</em> is the DB’s endpoint (i.e., the connection URL) <ul> <li>You cannot use a read replica as your source (it does not support on-going replication)</li> </ul></li> <li>The <em>User</em> and <em>Password</em> are for the master user account of the database <ul> <li>The account <em>needs</em> to be the master account. Otherwise, you need to follow the additional instructions listed in the <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.RDSPostgreSQL">documentation</a></li> <li>The <em>Password</em> cannot have certain characters (<code>: ; + %</code>)</li> </ul></li> <li>Test the endpoint <ul> <li>After a success ensure you refresh the schema</li> </ul></li> </ul> <h3>Create Task</h3> <p>The DMS <em>task</em> is where we actually configure and use the <em>endpoints</em> and <em>replication instance</em> to accomplish the migration. There are a couple of options and considerations to be made while creating a task. The following steps outline how to create a task, and the settings to configure:</p> <ul> <li>Use the <em>source and target endpoints</em> along with the <em>replica instance</em> that was created for this migration task</li> <li>Set the <em>Migration type</em> to <code>Migrate existing data and replicate ongoing changes</code> <ul> <li>This ensures that we are doing the 2 phase approach:</li> <li>Doing a full-load of the table&rsquo;s data</li> <li>Doing on-going replication of changes on the source database</li> </ul></li> <li><em>Target table preparation mode</em> should be <code>Do nothing</code> <ul> <li>The reason here is that we want to preserve the table&rsquo;s metadata (i.e., indexes, defaults, constraints)</li> <li><code>Truncate</code> could work, but is likely to fail due to constraints in the database</li> <li>Remember that by using <code>Do nothing</code> the target database needs to be truncated manually prior to running the task</li> </ul></li> <li><em>Stop task after full load completes</em> should be set to <code>Don&#39;t stop</code>. <ul> <li>This ensures the on-going replication process starts immediately when a table is fully loaded</li> </ul></li> <li>For the table mappings, we want to migrate all tables (i.e., <code>where schema name is like &#39;public&#39; and table name is like &#39;%&#39;, include</code>)</li> <li>It is useful to enable the <em>validation</em> and <em>logging</em> to see how things are progressing during the migration</li> </ul> <p><em>Include LOB columns in replication</em> is an interesting setting, LOBs are <em>Large Objects</em> that exist during the migration. As DMS is possibly migrating to a different database type a transformation of data types occurs. To understand what we&rsquo;re dealing with you need to look up the source database&rsquo;s supported types in the documentation. For example, using the DMS documentation you can see what <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.DataTypes">PostgreSQL data types</a> end up being LOBs.</p> <p>The following section will dig a little deeper into LOBs and the considerations that need to be considered.</p> <h4>Full or Limited LOB Mode</h4> <p>There are two options for handling LOBs during the migration: <em>Full</em> or <em>Limited</em> LOB Mode. LOBs are potentially massive objects that reside in the database, and they normally don&rsquo;t have a fixed size in the column. The following <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.LOBSupport.html">documentation</a> describes the two options:</p> <p>DMS provides you with the <em>Full LOB Mode</em>:</p> <blockquote> <p>In full LOB mode, AWS DMS migrates all LOBs from source to target regardless of size. In this configuration, AWS DMS has no information about the maximum size of LOBs to expect. Thus, LOBs are migrated one at a time, piece by piece. Full LOB mode can be quite slow.</p> </blockquote> <p>DMS also provides the <em>Limited LOB Mode</em>:</p> <blockquote> <p>In limited LOB mode, you set a maximum size LOB that AWS DMS should accept. Doing so allows AWS DMS to pre-allocate memory and load the LOB data in bulk. LOBs that exceed the maximum LOB size are truncated and a warning is issued to the log file. In limited LOB mode, you get significant performance gains over full LOB mode. We recommend that you use limited LOB mode whenever possible.</p> </blockquote> <p>Initially, it makes sense to just use <em>Full LOB Mode</em> as it&rsquo;ll preserve data by migrating LOBs regardless of size. The big issue with this choice is <em>speed</em>. In our migration, we achieve full migration in 2 hours using <em>Limited LOB Mode</em> and we estimated about 90 hours using <em>Full LOB Mode</em>. The issue with using <em>Limited LOB Mode</em> is that you can essentially lose data if the LOB&rsquo;s size is less than the <code>Max LOB Size</code> parameter.</p> <p>To work around this concern, the plan is to determine the max LOB size in the database and set the <code>Max LOB Size</code> to slightly bigger than that value (i.e., multiply it by 2). By using a larger <code>Max LOB Size</code> than the largest LOB we have in the database, we are ensured to have the full data migrated without any data loss.</p> <p>A manual way to check for the size of a LOB column is to use the following query: <code>SELECT max(pg_column_size(column_name)) FROM table_name;</code>. This will return the max number of bytes used in that column. This is the value you&rsquo;ll want to be larger than. The following is a Rails rake task that walks through every table&rsquo;s column and identifies the max sizes for any LOB column (for PostgreSQL):</p> <div class=highlight><pre class="highlight ruby"><code><span class="n">namespace</span> <span class="ss">:scripts</span> <span class="k">do</span>
  <span class="n">desc</span> <span class="s1">'Print out the max size of LOBs in the database. Usage: bin/rake scripts:max_lob_size'</span>
  <span class="n">task</span> <span class="ss">max_lob_size: :environment</span> <span class="k">do</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="no">LOB_TYPES</span> <span class="o">=</span> <span class="sx">%w(
      hstore
      composite
      array
      jsonb
      json
      polygon
      path
      xml
      tsquery
      tsvector
      bytea
      text
    )</span><span class="p">.</span><span class="nf">freeze</span>

    <span class="no">ActiveRecord</span><span class="o">::</span><span class="no">Base</span><span class="p">.</span><span class="nf">connection</span><span class="p">.</span><span class="nf">tables</span><span class="p">.</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">table_name</span><span class="o">|</span>
      <span class="k">next</span> <span class="k">if</span> <span class="n">table_name</span> <span class="o">==</span> <span class="s1">'schema_migrations'</span> <span class="o">||</span> <span class="n">table_name</span><span class="p">.</span><span class="nf">starts_with?</span><span class="p">(</span><span class="s1">'awsdms_'</span><span class="p">)</span>

      <span class="n">columns_sql</span> <span class="o">=</span> <span class="o">&lt;&lt;~</span><span class="no">HEREDOC</span><span class="sh">
        SELECT *
        FROM information_schema.columns
        WHERE table_schema = 'public'
          AND table_name   = '</span><span class="si">#{</span><span class="n">table_name</span><span class="si">}</span><span class="sh">'
</span><span class="no">      HEREDOC</span>

      <span class="n">columns_results</span> <span class="o">=</span> <span class="no">ActiveRecord</span><span class="o">::</span><span class="no">Base</span><span class="p">.</span><span class="nf">connection</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="n">columns_sql</span><span class="p">)</span>

      <span class="n">columns_results</span><span class="p">.</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">row</span><span class="o">|</span>
        <span class="n">column_name</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'column_name'</span><span class="p">]</span>
        <span class="n">column_type</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'data_type'</span><span class="p">]</span>

        <span class="k">if</span> <span class="no">LOB_TYPES</span><span class="p">.</span><span class="nf">include?</span><span class="p">(</span><span class="n">column_type</span><span class="p">)</span>
          <span class="n">size_sql</span> <span class="o">=</span> <span class="o">&lt;&lt;~</span><span class="no">HEREDOC</span><span class="sh">
            SELECT max(pg_column_size(</span><span class="si">#{</span><span class="n">column_name</span><span class="si">}</span><span class="sh">)) FROM </span><span class="si">#{</span><span class="n">table_name</span><span class="si">}</span><span class="sh">;
</span><span class="no">          HEREDOC</span>
          <span class="n">size_results</span> <span class="o">=</span> <span class="no">ActiveRecord</span><span class="o">::</span><span class="no">Base</span><span class="p">.</span><span class="nf">connection</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="n">size_sql</span><span class="p">)</span>

          <span class="n">table_hash</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="n">table_name</span><span class="p">.</span><span class="nf">to_sym</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
          <span class="n">column_hash</span> <span class="o">=</span> <span class="n">table_hash</span><span class="p">[</span><span class="n">column_name</span><span class="p">.</span><span class="nf">to_sym</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

          <span class="n">column_hash</span><span class="p">[</span><span class="ss">:column_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">column_type</span>
          <span class="n">column_hash</span><span class="p">[</span><span class="ss">:max_bytes_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">size_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'max'</span><span class="p">].</span><span class="nf">to_i</span>
        <span class="k">end</span>

        <span class="nb">print</span> <span class="s1">'.'</span>
      <span class="k">end</span>
    <span class="k">end</span>

    <span class="nb">puts</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Raw Dump"</span>
    <span class="n">pp</span> <span class="n">output</span>

    <span class="n">max_bytes</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">values</span><span class="p">.</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">x</span><span class="p">.</span><span class="nf">values</span><span class="p">.</span><span class="nf">first</span><span class="p">[</span><span class="ss">:max_bytes_size</span><span class="p">]</span> <span class="p">}.</span><span class="nf">compact</span><span class="p">.</span><span class="nf">max</span>
    <span class="nb">puts</span> <span class="s2">"Max bytes found in a LOB column is </span><span class="si">#{</span><span class="n">max_bytes</span><span class="si">}</span><span class="s2">"</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div> <h4>LOB Nullability Constraints</h4> <p>There are some concerns with <code>NOT NULL</code> database constraints on columns that become LOBs during the migration. DMS has a certain process for dealing with tables with LOBs:</p> <ul> <li>The data for each column in a row is added, except for LOB columns</li> <li>LOB columns are left with a <code>null</code> placeholder</li> <li>Afterwards, LOB data replaces the <code>null</code> placeholders</li> </ul> <p>This two-step process of dealing with LOB data types is where we have to be concerned with <code>NOT NULL</code> constraints. The migration will fail if any LOB columns have a <code>NOT NULL</code> constraint. In PostgreSQL, a common case of LOBs is <code>jsonb</code> and <code>array</code> columns. So we have to <em>remove</em> these constraints on the target database just until the full table load phase is done. For example, the following statement would do the trick, <code>ALTER TABLE my_table ALTER COLUMN lob_column DROP NOT NULL;</code>. Do not forget to add the <code>NOT NULL</code> constraints back after the full table load phase is done.</p> <h3>Migration Time</h3> <p>At this point, the migration process is pretty much ready! Let&rsquo;s break the process up into before/during/after migration.</p> <h4>Before Migration</h4> <p>Make sure that the:</p> <ul> <li>The DMS endpoints, task and replication instance are present and configured</li> <li>The Aurora instance is ready: <ul> <li>Truncated data</li> <li>Disabled LOB nullability constraints</li> </ul></li> </ul> <p>One thing that is suggested is to run <code>Assess</code> on the task so you can get a report of potential issues. In our case, there were a couple of <em>Partially supported datatypes : float8</em> on a few columns. This ended up changing the rounding of floats (i.e., 1.4999999998 &ndash;&gt; 1.5). It is worth noting that these differences occur after the migration process is completed and changes are being done on the new database type.</p> <p>For the migration, ideally, it is done during a <em>low</em> activity period. In addition, if possible stop any background jobs just before the migration, and wait for the current jobs to finish. The jobs can resume processing after the migration, this is to reduce risk.</p> <h4>During Migration</h4> <ul> <li>Keep an eye on the source database&rsquo;s health <ul> <li>There will be additional load placed on it during the migration</li> <li>If needed you could always lower the number of tables loaded in parallel during the <em>full load</em> phase (under the advanced settings in the <em>Task</em>)</li> </ul></li> <li>Monitor the task&rsquo;s <em>Table Statistics</em> tab to make sure tables are progressing well</li> <li>Monitor the task&rsquo;s <em>Task Monitoring</em> tab to make sure the on-going replication is keeping up <ul> <li>The <code>CDCIncomingChanges</code> should be as close to 0 as possible</li> <li>During the <em>full load</em> phase, the <code>CDCIncomingChanges</code> will climb as the on-going replication changes are stored until the tables have fully loaded into the target database</li> </ul></li> <li>Monitor the replication instance&rsquo;s <em>Monitoring</em> tab to make sure the <code>FreeStorageSpace</code> and <code>FreeableMemory</code> are not dropping too low <ul> <li>If any of these are too low then the migration can fail</li> </ul></li> </ul> <p>The first phase is <em>full table load</em>, where all the source data is dumped into the target. Be aware that large tables can take some time. After a table has been fully loaded into the target, the on-going replication phase starts for that table.</p> <p>The <em>on-going replication</em> phase is where <em>inserts/deletes/updates</em> are replicated from the source to the target. After all the tables are in this phase (all tables in a <em>Table Completed</em> state), it is now safe to re-enable the LOB nullability constraints that were disabled earlier.</p> <h5>Validations</h5> <p>If you have validations enabled for the <em>Task</em> then the <em>validation</em> columns in the <em>Table Statistics</em> will update during the migration. These validations put additional load on the source and target database as the row data is compared after that row has been migrated. It is an on-going validation process.</p> <p>Personally, I found the validation to be very flaky. It is either <em>really slow</em> and it also reports validation errors that are not actual errors. In our case, we didn&rsquo;t pay much attention to the validation failures as spot checking proved that the data was <em>fine</em>. There were <em>minor</em> cases where time columns were <em>slightly off</em>. I am unsure how the validation actually works (i.e., when it does the checks), as the on-going replication could be lagging behind. According to the <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html#CHAP_Validating.Limitations">documentation</a>, there are some limitations that might prevent it from being that useful:</p> <ul> <li>If one or more rows are being continuously modified during the validation, then AWS DMS can&rsquo;t validate those rows. However, you can validate those <em>busy</em> rows manually, after the task completes.</li> <li>If AWS DMS detects more than 10,000 failed or suspended records, it will stop the validation. Before you proceed further, resolve any underlying problems with the data.</li> </ul> <p>In either case under the target&rsquo;s database, there is a new table called <code>awsdms_validation_failures_v1</code> that contains information on the failures. The <code>KEY</code> and <code>TABLE_NAME</code> columns can be used to identify the record in question. It is then possible to check the source and target record and see if there are any issues. One other problem we had with spot-checking validation is that our PostgreSQL database used <code>UUID</code>s for primary keys, this resulted in the <code>KEY</code> column having truncated data on the <code>UUID</code>.</p> <h4>After Migration</h4> <p>Hopefully, the migration went successfully, and both the source and target database are in-sync. At this point, the zero-downtime migration can occur &ndash; simply point the application at the new database. It is advisable to wait for any replication lag or queued up <code>CDCIncomingChanges</code> to drain before proceeding. You might have to wait a little bit for the connections of your application to cycle over to the new database, but you can monitor this in CloudWatch, or force the cycle (i.e., restart Rails Unicorn servers). Don&rsquo;t forget to resume any background queues. After sufficient time, you can decommission the old database resources.</p> <p>In the event that the migration doesn&rsquo;t go as planned, it is possible to revert back to the old database. The only issue is that any write data that occurred during the migration process (i.e., only on the Aurora database) wouldn&rsquo;t be present on the old database (i.e., PostgreSQL database). This is simply an issue that cannot be worked around, and hopefully, the low activity period reduces the amount of data loss.</p> <div class=article__comments-button><button type=button class="btn btn-default show-comments">Click to Disqus</button></div> <div id=disqus_thread></div> </div> </div> </div> <script>
//<![CDATA[
    var disqus_shortname = 'kevinjalbert';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
//]]>
</script> </body> <div class="container footer"> <div class="col-md-12 footer__social"> <a href="https://twitter.com/KevinJalbert">Twitter</a> <a href="https://github.com/kevinjalbert">GitHub</a> <a href="https://stackoverflow.com/users/583592/kevin-jalbert">Stack Overflow</a> </div> <div class="col-md-12 footer__copywrite"> Code licensed <a href="https://github.com/kevinjalbert/kevinjalbert.github.io/blob/real-master/LICENSE">MIT</a>, Content &copy; 2019 Kevin Jalbert </div> </div> </html> <script src="/javascripts/app.js"></script>